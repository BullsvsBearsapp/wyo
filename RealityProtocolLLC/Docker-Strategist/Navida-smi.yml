# Verify NVIDIA drivers
nvidia-smi

# Check Docker GPU access
docker run --rm --gpus all nvidia/cuda:12.2.0-base nvidia-smi# Fix volume permissions
docker run --rm -v vllm-cache:/data busybox chown -R 1000:1000 /data# Pre-download model manually
docker run --rm -it \
  -e HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN} \
  -v vllm-cache:/root/.cache/huggingface \
  pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime \
  python -c "from transformers import AutoModel; AutoModel.from_pretrained('NousResearch/Hermes-2-Pro-Mistral-7B')"version: '3.8'

services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    command: [
      "--model", "NousResearch/Hermes-2-Pro-Mistral-7B",
      "--tensor-parallel-size", "4",
      "--gpu-memory-utilization", "0.95"
    ]
    resources:
      limits:
        memory: 120g# Check container resource usage
docker stats $(docker compose -f vllm-agent.yml ps -q)

# Test API endpoint
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "NousResearch/Hermes-2-Pro-Mistral-7B",
    "prompt": "Explain quantum computing in simple terms",
    "max_tokens": 100
  }'
